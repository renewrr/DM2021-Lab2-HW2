{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "td_route = \"./dm2021-lab2-hw2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_list = []\n",
    "with open(td_route + '/tweets_DM.json') as f:\n",
    "    for jsonObj in tqdm(f):\n",
    "        Tweet_list.append(json.loads(jsonObj))\n",
    "#I use tqdm to check the progress of many operations in this project\n",
    "#since the dataset is very large, and I want to see how long the task\n",
    "#would take to complete so I don't have to sit and stare at nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_Tweet_list = Tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(True):\n",
    "    Tweet_list = ori_Tweet_list[len(ori_Tweet_list)//10*0:len(ori_Tweet_list)//10*1]\n",
    "#The dataset is too large so I broke it into 10 parts and rotate them each epoch\n",
    "#The rotation was done manually as each epoch took really long and automation doesn't feel necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification = {'test':set(),'train':set()}\n",
    "with open(td_route + '/data_identification.csv', newline='') as csvfile:\n",
    "    rd = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for tid,ident in rd:\n",
    "        if(ident in identification):identification[ident].add(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {}\n",
    "with open(td_route + '/emotion.csv', newline='') as csvfile:\n",
    "    rd = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(rd, None) #Skip header\n",
    "    for tid,emt in rd:\n",
    "        emotions[tid] = emt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_dict = {'train':{},'test':{}}\n",
    "for tl in Tweet_list:\n",
    "    tid = tl['_source']['tweet']['tweet_id']\n",
    "    if(tid in identification['train']):\n",
    "        Tweet_dict['train'][tid] = tl['_source']['tweet']['text']\n",
    "    elif(tid in identification['test']):\n",
    "        Tweet_dict['test'][tid] = tl['_source']['tweet']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Tweet_dict['train']),len(Tweet_dict['test']))\n",
    "print(len(emotions))\n",
    "print(len(Tweet_list),len(ori_Tweet_list))\n",
    "#Checks for dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#Pretrained BertTokenizer were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenz = tokenizer(Tweet_dict['train']['0x376b20'],padding='max_length', max_length = 280, truncation=True,return_tensors=\"pt\")\n",
    "tokenz\n",
    "#tokenizer operation check, since The max length of a tweet is 280 characters, I set that as the length of vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for tl in ori_Tweet_list:\n",
    "    if(len(tl['_source']['tweet']['text']) > max_len):\n",
    "        max_len = len(tl['_source']['tweet']['text'])\n",
    "print(max_len)\n",
    "#Double check maximum length, which turns out is 252\n",
    "#I still used 280 as max length in the end to make sure no additional data would break this code in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'anger':0,'anticipation':1,'disgust':2,'fear':3,'sadness':4,'surprise':5,'trust':6,'joy':7}\n",
    "#lable dict to convert text to class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts):\n",
    "        self.labels = [labels[emotions[key]] for key in tqdm(texts)]\n",
    "        self.texts = [tokenizer(texts[key],padding='max_length', max_length = 280, truncation=True,return_tensors=\"pt\") for key in tqdm(texts)]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        #Start with a pretrained model to speed up training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 8)\n",
    "        #Classification layer has 8 outputs since we have 8 classes of emotions\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = Dataset(Tweet_dict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(full_ds)*0.7)\n",
    "test_size = int(len(full_ds)*0.3)\n",
    "if(len(full_ds) % 10 != 0):test_size += 1\n",
    "train_ds,test_ds = torch.utils.data.random_split(full_ds, [train_size, test_size], generator=torch.Generator().manual_seed(42069))\n",
    "#Split the dataset into training and testing sets, with a 7:3 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ds),len(test_ds),len(train_ds)/len(full_ds),len(test_ds)/len(full_ds))\n",
    "#Dataset sizes validation\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operation Checks\n",
    "batch_iterator = iter(train_dataloader)\n",
    "inputs, label = next(batch_iterator)\n",
    "for inputs,label in tqdm(batch_iterator):\n",
    "    continue\n",
    "print(label)\n",
    "del batch_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, learning_rate, epochs):\n",
    "\n",
    "    train,val = train_ds,test_ds\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    cnter = 12\n",
    "\n",
    "    if use_cuda:\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.type(torch.LongTensor).to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                #print(output,train_label)\n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in tqdm(val_dataloader):\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_ds): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_ds): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(test_ds): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(test_ds): .3f}')\n",
    "            torch.save(model.state_dict(), 'best_checkpoint_1222_'+ str(cnter) +'.pth')\n",
    "            cnter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "model = BertClassifier()\n",
    "model.load_state_dict(torch.load('best_checkpoint_1222_11.pth'))\n",
    "LR = 1e-6\n",
    "train(model, LR, EPOCHS)\n",
    "#The state file of the Bert model is too large to include, google drive link provided\n",
    "#In the end I stop at each epoch since even with 1/10 of the data, it still takes 2 hour to train each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "for tl in ori_Tweet_list:\n",
    "    tid = tl['_source']['tweet']['tweet_id']\n",
    "    if(tid in identification['test']):\n",
    "        test_dict[tid] = tl['_source']['tweet']['text']\n",
    "print(len(test_dict))\n",
    "\n",
    "#Get validation tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_labels = {}\n",
    "for key in labels:\n",
    "    rev_labels[labels[key]] = key\n",
    "    \n",
    "#reverse label dict since the output is in class_id and not text like 'anticipation' that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for key in tqdm(test_dict):\n",
    "        val_input = tokenizer(test_dict[key],padding='max_length', max_length = 280, truncation=True,return_tensors=\"pt\")\n",
    "        mask = val_input['attention_mask'].to(device)\n",
    "        input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "        output = model(input_id, mask).to('cpu')\n",
    "        outs.append([key,rev_labels[int(torch.argmax(output))]])\n",
    "\n",
    "#Generate validation labels, this took 3 hours each run :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "print(len(outs))\n",
    "with open('val3.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow([\"id\"]+[\"emotion\"])\n",
    "    for row in outs:  \n",
    "        spamwriter.writerow(row)\n",
    "#Write output file with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##These code was copied from another notebook file, the original notebook named HW2.ipynb is also included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Report##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I heard from my final project teammates that Bert performs pretty well at this task, I did not do my own feature engineering and model development, so most of my time spent on this project was monitoring the progress of the training, which is really slow, and tweak the input size and learning rate if overfitting occurs. The only data preprocessing I did was to split the data into 10 parts since training the whole dataset at once max out my computer's memory. When the model stops improving I increased the dataset size and the batch size, which did imporve a bit more but progress was slow, and I can't increase the batch size too much as my GPU's memory was insufficient, in the end I stopped at 0.52 F-score. Later I learned that the training process I used is basically Early stopping, I start doing this when the model overfits after a ten hour training of 3 epochs, to stop wasting time I only train 1 epoch at a time after that, and check the progress at each step to determine if tweaks are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af595d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "td_route = \"./dm2021-lab2-hw2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6720355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2619201ae5047d59e18d755d7f483a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tweet_list = []\n",
    "with open(td_route + '/tweets_DM.json') as f:\n",
    "    for jsonObj in tqdm(f):\n",
    "        Tweet_list.append(json.loads(jsonObj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51507a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_Tweet_list = Tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf0027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(True):\n",
    "    Tweet_list = ori_Tweet_list[len(ori_Tweet_list)//10*0:len(ori_Tweet_list)//10*5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8974575",
   "metadata": {},
   "outputs": [],
   "source": [
    "identification = {'test':set(),'train':set()}\n",
    "with open(td_route + '/data_identification.csv', newline='') as csvfile:\n",
    "    rd = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for tid,ident in rd:\n",
    "        if(ident in identification):identification[ident].add(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbade95",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {}\n",
    "with open(td_route + '/emotion.csv', newline='') as csvfile:\n",
    "    rd = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(rd, None) #Skip header\n",
    "    for tid,emt in rd:\n",
    "        emotions[tid] = emt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3bd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_dict = {'train':{},'test':{}}\n",
    "for tl in Tweet_list:\n",
    "    tid = tl['_source']['tweet']['tweet_id']\n",
    "    if(tid in identification['train']):\n",
    "        Tweet_dict['train'][tid] = tl['_source']['tweet']['text']\n",
    "    elif(tid in identification['test']):\n",
    "        Tweet_dict['test'][tid] = tl['_source']['tweet']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ef3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727734 206031\n",
      "1455563\n",
      "933765 1867535\n"
     ]
    }
   ],
   "source": [
    "print(len(Tweet_dict['train']),len(Tweet_dict['test']))\n",
    "print(len(emotions))\n",
    "print(len(Tweet_list),len(ori_Tweet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da15d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')    # Download vocabulary from S3 and cache\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61275528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased', output_attentions=True)  # Update configuration during loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7950918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2563,  1150,  2112,   107,  5194,  1143,  1113,   108,   156,\n",
       "         27864,  7147,  1204,   107,  1538,  1129,  1260,  7889,  7412,  1906,\n",
       "           119,   140, 14875,  1299,   119,   119,   119,   119,  1115,   112,\n",
       "           188,   133,   149,  3048,   135,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#indexed_tokens = tokenizer.encode(Tweet_dict['train']['0x376b20'], add_special_tokens=True)\n",
    "tokenz = tokenizer(Tweet_dict['train']['0x376b20'],padding='max_length', max_length = 280, truncation=True,return_tensors=\"pt\")\n",
    "tokenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a305321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'anger':0,'anticipation':1,'disgust':2,'fear':3,'sadness':4,'surprise':5,'trust':6,'joy':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d733260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts):\n",
    "        self.labels = [labels[emotions[key]] for key in tqdm(texts)]\n",
    "        self.texts = [tokenizer(texts[key],padding='max_length', max_length = 512, truncation=True,return_tensors=\"pt\") for key in tqdm(texts)]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10821180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1fd7159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7d5a5fb0b342d79cd0a3e51f75e9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/727734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5520c0abf3bc422187f253b9abe57169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/727734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_ds = Dataset(Tweet_dict['train'])\n",
    "#train_ds,test_ds = random_split(full_ds, [3, 7], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a7557b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(full_ds, 'dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb1ef3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(full_ds)*0.7)\n",
    "test_size = int(len(full_ds)*0.3)\n",
    "if(len(full_ds) % 10 != 0):test_size += 1\n",
    "train_ds,test_ds = torch.utils.data.random_split(full_ds, [train_size, test_size], generator=torch.Generator().manual_seed(42069))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5fc6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509413 218321 0.6999989006972328 0.3000010993027672\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds),len(test_ds),len(train_ds)/len(full_ds),len(test_ds)/len(full_ds))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "167944a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 3])\n"
     ]
    }
   ],
   "source": [
    "#Operation Checks\n",
    "batch_iterator = iter(train_dataloader)\n",
    "inputs, label = next(batch_iterator)\n",
    "#for inputs,label in tqdm(batch_iterator):\n",
    "    #continue\n",
    "print(label)\n",
    "del batch_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05af1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, learning_rate, epochs):\n",
    "\n",
    "    #train, val = Dataset(train_data), Dataset(val_data)\n",
    "    train,val = train_ds,test_ds\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    #use_cuda = False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    cnter = 12\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.type(torch.LongTensor).to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                #print(output,train_label)\n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in tqdm(val_dataloader):\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_ds): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_ds): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(test_ds): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(test_ds): .3f}')\n",
    "            torch.save(model.state_dict(), 'best_checkpoint_1222_'+ str(cnter) +'.pth')\n",
    "            cnter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0451db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "#model = BertClassifier()\n",
    "#model.load_state_dict(torch.load('best_checkpoint_1222_11.pth'))\n",
    "LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e27343fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f1665f369c4a07b2a61b3b0f25eefb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/254707 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bad43250ac24c5fb73feaaaab196acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.430                 | Train Accuracy:  0.701                 | Val Loss:  0.428                 | Val Accuracy:  0.701\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "train(model, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6731457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 30 01:49:34 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.13       Driver Version: 496.13       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 28%   40C    P8     9W / 180W |   5293MiB /  8192MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1184    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1196    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      1228    C+G   ...p-2.9.6\\GitHubDesktop.exe    N/A      |\n",
      "|    0   N/A  N/A      1444    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8244    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A      8672    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      9940    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10132    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10956    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     11396    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12316    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     12336    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13084    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13276    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13612    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     14264    C+G   ...\\app-1.0.9003\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     14852    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     14980    C+G   ...root\\Office16\\WINWORD.EXE    N/A      |\n",
      "|    0   N/A  N/A     15404    C+G   ...3.0.8.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     17168    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     19240    C+G   ...LINE\\bin\\current\\LINE.exe    N/A      |\n",
      "|    0   N/A  N/A     20152      C   ...yTorch_new_cv2\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "457a6bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772cb2cd86de41e792db44970b195989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tweet_list2 = []\n",
    "with open(td_route + '/tweets_DM.json') as f:\n",
    "    for jsonObj in tqdm(f):\n",
    "        Tweet_list2.append(json.loads(jsonObj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81d8f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411972\n"
     ]
    }
   ],
   "source": [
    "test_dict = {}\n",
    "for tl in ori_Tweet_list:\n",
    "    tid = tl['_source']['tweet']['tweet_id']\n",
    "    if(tid in identification['test']):\n",
    "        test_dict[tid] = tl['_source']['tweet']['text']\n",
    "print(len(test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6135f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_labels = {}\n",
    "for key in labels:\n",
    "    rev_labels[labels[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac4a3246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054d3732d8f34412aa819f7270cbb9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/411972 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for key in tqdm(test_dict):\n",
    "        val_input = tokenizer(test_dict[key],padding='max_length', max_length = 280, truncation=True,return_tensors=\"pt\")\n",
    "        mask = val_input['attention_mask'].to(device)\n",
    "        input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "        output = model(input_id, mask).to('cpu')\n",
    "        #print(torch.argmax(output))\n",
    "        outs.append([key,rev_labels[int(torch.argmax(output))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e60ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411972\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "print(len(outs))\n",
    "with open('val3.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow([\"id\"]+[\"emotion\"])\n",
    "    for row in outs:  \n",
    "        spamwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "102e6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411972\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf0b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
